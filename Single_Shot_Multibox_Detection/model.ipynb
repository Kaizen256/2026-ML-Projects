{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba39c0c",
   "metadata": {},
   "source": [
    "# Single Shot Multibox Detection\n",
    "\n",
    "I was having difficulties with RandomIoUCrop, It wasn't working great for various reasons, the resulting images had bad GT boxes, or just timed out due to not being able to get a high enough IoU with a GT box. There are multiple fixes like adding in a sampler_option = 0.0, so it's never impossible, but I still found the resulting GT boxes were not great, you would get images clearly containing people, without any GT box. So I got Chat GPT to create a class that centers crops around a GT box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a3161b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2 as T\n",
    "from torch import nn, optim\n",
    "\n",
    "def _area_xyxy(b): \n",
    "    return (b[:,2]-b[:,0]).clamp(0) * (b[:,3]-b[:,1]).clamp(0)\n",
    "\n",
    "def _clip_to_canvas(boxes, H, W):\n",
    "    x1 = boxes[:,0].clamp(0, W-1)\n",
    "    y1 = boxes[:,1].clamp(0, H-1)\n",
    "    x2 = boxes[:,2].clamp(0, W-1)\n",
    "    y2 = boxes[:,3].clamp(0, H-1)\n",
    "    # ensure x2>=x1, y2>=y1 after clamp\n",
    "    x2 = torch.maximum(x2, x1)\n",
    "    y2 = torch.maximum(y2, y1)\n",
    "    return torch.stack([x1,y1,x2,y2], dim=1)\n",
    "\n",
    "class SafeBoxCenteredCrop(nn.Module):\n",
    "    \"\"\"\n",
    "    Object-centric crop around a randomly chosen GT box,\n",
    "    with visibility/AR/scale checks and bounded retries.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_size=(300,300),\n",
    "        context=(1.2, 2.0),           # crop size = context * target box size\n",
    "        scale_img=(0.3, 1.0),         # crop area as fraction of image area (feasibility guard)\n",
    "        ratio=(0.6, 1.8),             # crop aspect ratio bounds (w/h)\n",
    "        max_trials=30,\n",
    "        target_vis_thresh=0.7,        # target box must be at least 70% visible\n",
    "        keep_vis_thresh=0.5,          # other boxes kept if ≥50% visible\n",
    "        antialias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_size = out_size\n",
    "        self.context = context\n",
    "        self.scale_img = scale_img\n",
    "        self.ratio = ratio\n",
    "        self.max_trials = max_trials\n",
    "        self.target_vis_thresh = target_vis_thresh\n",
    "        self.keep_vis_thresh = keep_vis_thresh\n",
    "        self.antialias = antialias\n",
    "\n",
    "    def forward(self, sample):\n",
    "        img: tv_tensors.Image = sample[\"image\"]\n",
    "        boxes: tv_tensors.BoundingBoxes = sample[\"boxes\"]\n",
    "        labels: torch.Tensor = sample[\"labels\"]\n",
    "\n",
    "        C, H, W = img.shape\n",
    "        if boxes.numel() == 0:\n",
    "            # no boxes → fallback to simple resize\n",
    "            return T.Resize(self.out_size, antialias=self.antialias)(sample)\n",
    "\n",
    "        img_area = float(H * W)\n",
    "        b = boxes.as_subclass(torch.Tensor)  # (N,4) float xyxy\n",
    "        areas = _area_xyxy(b)\n",
    "        # pick a target box proportional to sqrt(area) to bias toward larger persons\n",
    "        weights = (areas.clamp(min=1)).sqrt()\n",
    "        tidx = torch.multinomial(weights, 1).item()\n",
    "\n",
    "        for _ in range(self.max_trials):\n",
    "            tb = b[tidx]  # target box\n",
    "            bw = (tb[2]-tb[0]).item()\n",
    "            bh = (tb[3]-tb[1]).item()\n",
    "            if bw < 1 or bh < 1:\n",
    "                continue\n",
    "\n",
    "            # sample context scale\n",
    "            s = torch.empty(1).uniform_(*self.context).item()\n",
    "            crop_w = max(1.0, s * bw)\n",
    "            crop_h = max(1.0, s * bh)\n",
    "\n",
    "            # optional AR jitter within bounds\n",
    "            ar_min, ar_max = self.ratio\n",
    "            # adjust width/height to satisfy aspect ratio softly\n",
    "            ar = crop_w / crop_h\n",
    "            if ar < ar_min:\n",
    "                crop_w = ar_min * crop_h\n",
    "            elif ar > ar_max:\n",
    "                crop_h = crop_w / ar_max\n",
    "\n",
    "            # make sure crop area is not absurd vs image\n",
    "            crop_area = crop_w * crop_h\n",
    "            if not (self.scale_img[0]*img_area <= crop_area <= self.scale_img[1]*img_area):\n",
    "                # rescale to nearest bound\n",
    "                target_area = min(max(crop_area, self.scale_img[0]*img_area), self.scale_img[1]*img_area)\n",
    "                scale = (target_area / (crop_w*crop_h))**0.5\n",
    "                crop_w *= scale\n",
    "                crop_h *= scale\n",
    "\n",
    "            # choose crop center near target box center with some jitter\n",
    "            cx = (tb[0] + tb[2]) / 2\n",
    "            cy = (tb[1] + tb[3]) / 2\n",
    "            jx = (torch.randn(1).item() * 0.15) * bw  # 15% bw jitter\n",
    "            jy = (torch.randn(1).item() * 0.15) * bh  # 15% bh jitter\n",
    "            cx = (cx + jx).clamp(0, W-1)\n",
    "            cy = (cy + jy).clamp(0, H-1)\n",
    "\n",
    "            x1 = (cx - crop_w/2); y1 = (cy - crop_h/2)\n",
    "            x2 = (cx + crop_w/2); y2 = (cy + crop_h/2)\n",
    "\n",
    "            crop = torch.tensor([[x1,y1,x2,y2]], dtype=b.dtype, device=b.device)\n",
    "            crop = _clip_to_canvas(crop, H, W)[0]\n",
    "\n",
    "            # compute visibility of each box inside crop\n",
    "            inter = torch.stack([\n",
    "                torch.maximum(b[:,0], crop[0]),\n",
    "                torch.maximum(b[:,1], crop[1]),\n",
    "                torch.minimum(b[:,2], crop[2]),\n",
    "                torch.minimum(b[:,3], crop[3]),\n",
    "            ], dim=1)\n",
    "            inter = _clip_to_canvas(inter, H, W)\n",
    "            inter_area = _area_xyxy(inter)\n",
    "            box_area = _area_xyxy(b).clamp(min=1)\n",
    "            vis_frac = (inter_area / box_area)  # per-box visible fraction\n",
    "\n",
    "            # require target box visibility\n",
    "            if vis_frac[tidx].item() < self.target_vis_thresh:\n",
    "                continue\n",
    "\n",
    "            # keep boxes with enough visibility\n",
    "            keep = vis_frac >= self.keep_vis_thresh\n",
    "            if keep.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # translate+clip kept boxes to crop frame\n",
    "            kb = b[keep].clone()\n",
    "            kb[:,0::2] = kb[:,0::2].clamp(crop[0], crop[2]) - crop[0]\n",
    "            kb[:,1::2] = kb[:,1::2].clamp(crop[1], crop[3]) - crop[1]\n",
    "\n",
    "            # apply crop to image\n",
    "            x1i, y1i, x2i, y2i = crop.tolist()\n",
    "            x1i, y1i, x2i, y2i = int(x1i), int(y1i), int(x2i), int(y2i)\n",
    "            img_c = img[:, y1i:y2i, x1i:x2i]\n",
    "\n",
    "            # resize to output\n",
    "            resize = T.Resize(self.out_size, antialias=self.antialias)\n",
    "            img_c = resize(tv_tensors.Image(img_c))\n",
    "            # resize boxes accordingly\n",
    "            inH, inW = y2i - y1i, x2i - x1i\n",
    "            scale_x = self.out_size[1] / max(1, inW)\n",
    "            scale_y = self.out_size[0] / max(1, inH)\n",
    "            kb[:, [0,2]] *= scale_x\n",
    "            kb[:, [1,3]] *= scale_y\n",
    "\n",
    "            out = {\n",
    "                \"image\": img_c,\n",
    "                \"boxes\": tv_tensors.BoundingBoxes(\n",
    "                    kb, format=tv_tensors.BoundingBoxFormat.XYXY, canvas_size=self.out_size\n",
    "                ),\n",
    "                \"labels\": labels[keep],\n",
    "            }\n",
    "            return out\n",
    "\n",
    "        # Fallback if no candidate passed quality in max_trials\n",
    "        fallback = T.Compose([\n",
    "            T.RandomResizedCrop(size=self.out_size, scale=(0.8,1.0), ratio=(0.75,1.5), antialias=True)\n",
    "        ])\n",
    "        out = fallback({\"image\": img, \"boxes\": boxes, \"labels\": labels})\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a0bc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudan(Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.root = root\n",
    "        self.imgs  = sorted(os.listdir(os.path.join(root, \"PNGImages\")))\n",
    "        self.masks = sorted(os.listdir(os.path.join(root, \"PedMasks\")))\n",
    "        # SSD pipeline: photometric --> geometric --> bbox cleanup --> float\n",
    "        if train:\n",
    "            self.tf = T.Compose([\n",
    "                T.ToImage(),\n",
    "                T.RandomPhotometricDistort(),\n",
    "                T.RandomChoice([\n",
    "                    SafeBoxCenteredCrop(out_size=(300,300),\n",
    "                                        context=(1.3, 2.2),\n",
    "                                        scale_img=(0.25, 0.95),\n",
    "                                        ratio=(0.6, 1.8),\n",
    "                                        max_trials=30,\n",
    "                                        target_vis_thresh=0.7,\n",
    "                                        keep_vis_thresh=0.5),\n",
    "                    T.RandomResizedCrop(size=(300,300), scale=(0.85,1.0), ratio=(0.75,1.5), antialias=True),\n",
    "                    T.Identity(),\n",
    "                ], p=[0.5, 0.3, 0.2]),\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.SanitizeBoundingBoxes(min_size=2),\n",
    "                T.ToDtype(torch.float32, scale=True),\n",
    "            ])\n",
    "        else:\n",
    "            self.tf = T.Compose([\n",
    "                T.ToImage(),\n",
    "                T.Resize((300, 300), antialias=True),\n",
    "                T.SanitizeBoundingBoxes(min_size=1),\n",
    "                T.ToDtype(torch.float32, scale=True),\n",
    "            ])\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ip = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mp = os.path.join(self.root, \"PedMasks\",  self.masks[idx])\n",
    "\n",
    "        img  = read_image(ip)                  # (3,H,W), uint8\n",
    "        mask = read_image(mp)[0]               # (H,W), instance ids\n",
    "        ids  = torch.unique(mask)[1:]          # drop background=0\n",
    "        masks = (mask[None] == ids[:, None, None]).to(torch.uint8)  # (N,H,W)\n",
    "        boxes = masks_to_boxes(masks)          # (N,4) xyxy on original size\n",
    "\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)   # 1 = person\n",
    "\n",
    "        img = tv_tensors.Image(img)\n",
    "        boxes = tv_tensors.BoundingBoxes(\n",
    "            boxes, format=tv_tensors.BoundingBoxFormat.XYXY,\n",
    "            canvas_size=img.shape[-2:]\n",
    "        )\n",
    "\n",
    "        sample = {\"image\": img, \"boxes\": boxes, \"labels\": labels}\n",
    "        sample = self.tf(sample)\n",
    "\n",
    "        out = {\n",
    "            \"boxes\":  torch.as_tensor(sample[\"boxes\"], dtype=torch.float32),\n",
    "            \"labels\": sample[\"labels\"]\n",
    "        }\n",
    "        return sample[\"image\"], out\n",
    "\n",
    "def collate(batch):\n",
    "    imgs, targets = list(zip(*batch))\n",
    "    return torch.stack(imgs, 0), list(targets)\n",
    "\n",
    "root = \"PennFudanPed\"\n",
    "train_ds = PennFudan(root, train=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,\n",
    "                          num_workers=0, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96500343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "for i in range(0, 6):\n",
    "    print(i)\n",
    "    img, label = train_ds[i]\n",
    "\n",
    "    img_uint8 = (img * 255).to(torch.uint8)  \n",
    "\n",
    "    out = draw_bounding_boxes(img_uint8, label[\"boxes\"], colors=\"red\", width=2)\n",
    "    pil_out = TF.to_pil_image(out)\n",
    "    pil_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc8ee3",
   "metadata": {},
   "source": [
    "![Img1](figures/img1.png)\n",
    "![Img2](figures/img2.png)\n",
    "![Img3](figures/img3.png)\n",
    "![Img4](figures/img4.png)\n",
    "![Img5](figures/img5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0bf0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = resnet34(weights=\"IMAGENET1K_V1\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4ebcd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49c39f",
   "metadata": {},
   "source": [
    "### Original Architecture\n",
    "\n",
    "| Stage    | Operation                        | Output Shape (C × H × W) |\n",
    "| -------- | -------------------------------- | ------------------------ |\n",
    "| Input    | —                                | 3 × 300 × 300            |\n",
    "| Conv1    | 7×7, stride 2, pad 3             | 64 × 150 × 150           |\n",
    "| MaxPool  | 3×3, stride 2, pad 1             | 64 × 75 × 75             |\n",
    "| Layer 1  | 3 blocks, stride 1               | 64 × 75 × 75             |\n",
    "| Layer 2  | 4 blocks, stride 2 (first block) | 128 × 38 × 38            |\n",
    "| Layer 3  | 6 blocks, stride 2 (first block) | 256 × 19 × 19            |\n",
    "| Layer 4  | 3 blocks, stride 2 (first block) | 512 × 10 × 10            |\n",
    "| AvgPool  | global (kernel = 10×10)          | 512 × 1 × 1              |\n",
    "| FC       | 1000 (ImageNet)                  | —                        |\n",
    "\n",
    "### Adjusted Architecture\n",
    "\n",
    "| Stage    | Operation                        | Output Shape (C × H × W) | Anchors |\n",
    "| -------- | -------------------------------- | ------------------------ |---------|\n",
    "| Input    | —                                | 3 × 300 × 300            | No      |\n",
    "| Conv1    | 7×7, stride 2, pad 3             | 64 × 150 × 150           | No      |\n",
    "| MaxPool  | 3×3, stride 2, pad 1             | 64 × 75 × 75             | No      |\n",
    "| Layer 1  | 3 blocks, stride 1               | 64 × 75 × 75             | No      |\n",
    "| Layer 2  | 4 blocks, stride 2 (first block) | 128 × 38 × 38            | Yes     |\n",
    "| Layer 3  | 6 blocks, stride 2 (first block) | 256 × 19 × 19            | Yes     |\n",
    "| Layer 4  | 3 blocks, stride 2 (first block) | 512 × 10 × 10            | Yes     |\n",
    "| C5       | Conv2d(512, 256, ks=3, s=2, p=1) | 256 × 5 × 5              | Yes     |\n",
    "| C6       | Conv2d(256, 256, ks=3, s=2, p=1) | 256 × 3 × 3              | Yes     |\n",
    "| C7       | Conv2d(256, 256, ks=3, s=2, p=1) | 256 × 1 × 1              | Yes     |\n",
    "\n",
    "\n",
    "\n",
    "| Map         | in\\_ch | mid\\_ch  | out\\_ch | Why       |\n",
    "| ------------| -----| ----| ----| -------------------- |\n",
    "| C5 (10-->5) | 512  | 256 | 256 |-                     |\n",
    "| C6 (5-->3)  | 256  | 128 | 256 |-                     |\n",
    "| C7 (3-->1)  | 256  | -   | 256 | Single 3×3, s=1, p=0 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3b32c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "def ds_block(in_ch, mid_ch, out_ch):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, mid_ch, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(mid_ch),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(mid_ch, out_ch, 3, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f2bf4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        bone = resnet34(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "        self.stem = nn.Sequential(bone.conv1, \n",
    "                                  bone.bn1,\n",
    "                                  bone.relu, \n",
    "                                  bone.maxpool\n",
    "                                  )\n",
    "        \n",
    "        self.layer1 = bone.layer1\n",
    "        self.layer2 = bone.layer2\n",
    "        self.layer3 = bone.layer3\n",
    "        self.layer4 = bone.layer4\n",
    "\n",
    "        self.c5 = ds_block(512, 256, 256)\n",
    "        self.c6 = ds_block(256, 128, 256)\n",
    "        self.c7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.out_l2 = nn.LazyConv2d(256, 1, bias=False)\n",
    "        self.out_l3 = nn.LazyConv2d(256, 1, bias=False)\n",
    "        self.out_l4 = nn.LazyConv2d(256, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)          # 64 x 75 x 75\n",
    "        o2 = self.layer1(x)       # 64 x 75 x 75\n",
    "        o3 = self.layer2(o2)      # 128 x 38 x 38\n",
    "        o4 = self.layer3(o3)      # 256 x 19 x 19\n",
    "        o5 = self.layer4(o4)      # 512 x 10 x 10\n",
    "\n",
    "        f3 = self.out_l2(o3)      # 256 x 38 x 38\n",
    "        f4 = self.out_l3(o4)      # 256 x 19 x 19\n",
    "        f5 = self.out_l4(o5)      # 256 x 10 x 10\n",
    "\n",
    "        f6 = self.c5(o5)          # 256 x 5 x 5\n",
    "        f7 = self.c6(f6)          # 256 x 3 x 3\n",
    "        f8 = self.c7(f7)          # 256 x 1 x 1\n",
    "\n",
    "        return [f3, f4, f5, f6, f7, f8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e99d87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rowek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f3: (1, 256, 38, 38)\n",
      "f4: (1, 256, 19, 19)\n",
      "f5: (1, 256, 10, 10)\n",
      "f6: (1, 256, 5, 5)\n",
      "f7: (1, 256, 3, 3)\n",
      "f8: (1, 256, 1, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, label = train_ds[0]\n",
    "model = ResNet34Backbone().eval()\n",
    "with torch.no_grad():\n",
    "    maps = model(img.unsqueeze(0))\n",
    "\n",
    "for i, f in enumerate(maps, 3):\n",
    "    print(f\"f{i}: {tuple(f.shape)}\")\n",
    "\n",
    "fig, axs = plt.subplots(1, len(maps), figsize=(20, 4))\n",
    "for i, f in enumerate(maps):\n",
    "    fmap = f[0, 0].detach().cpu()\n",
    "    axs[i].imshow(fmap, cmap=\"viridis\")\n",
    "    axs[i].set_title(f\"f{i+3} {f.shape[2]}x{f.shape[3]}\")\n",
    "    axs[i].axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb9bfe",
   "metadata": {},
   "source": [
    "![Feature Maps](figures/fmaps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e517290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
