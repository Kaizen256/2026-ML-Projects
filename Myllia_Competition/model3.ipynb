{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e760ba",
   "metadata": {},
   "source": [
    "# Myllia Model: ctrl-SVD + GenePT(m3) + external_sig + TFIDF(text)\n",
    "Output: Weighted-output PCA -> KernelRidge(RBF)  \n",
    "Hyperparam search: LOOCV proxy using official-metric math (approx)  \n",
    "Writes a submission CSV at the end  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a98519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, pickle, zipfile, urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2288ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 6\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MEANS_PATH = \"data/training_data_means.csv\"\n",
    "VALMAP_PATH = \"data/pert_ids_val.csv\"\n",
    "H5AD_PATH   = \"Data/training_cells.h5ad\"\n",
    "\n",
    "GENEPT_DIR = \"external_genept\"\n",
    "GENEPT_SUBDIR = os.path.join(GENEPT_DIR, \"GenePT_emebdding_v2\")\n",
    "SIG_NPZ = os.path.join(\"external_sig\", \"external_sig_k562_essential.npz\")\n",
    "\n",
    "GENEPT_ZENODO_URL = \"https://zenodo.org/records/10833191/files/GenePT_emebdding_v2.zip?download=1\"\n",
    "GENEPT_ZIP = os.path.join(GENEPT_DIR, \"GenePT.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5329dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dims\n",
    "CTRL_SVD_DIM     = 128\n",
    "GENEPT_PCA_DIM   = 128\n",
    "SIG_DIM_EXPECTED = None  # inferred from npz\n",
    "TFIDF_SVD_DIM    = 128\n",
    "\n",
    "# Control trimming\n",
    "DEPTH_TRIM_Q = 0.99\n",
    "\n",
    "# Search grids\n",
    "OUT_K_GRID    = [32, 48, 64, 80]\n",
    "ALPHA_GRID    = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "GAMMA_GRID    = [0.01, 0.02, 0.05, 0.1, 0.2]\n",
    "\n",
    "# Post grids (best was scale=0.85, thr=0.0 from model 2)\n",
    "SCALE_GRID = [0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "THR_GRID   = [0.0, 0.01, 0.02, 0.05, 0.08]\n",
    "\n",
    "BLEND_GRID = [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0f618",
   "metadata": {},
   "source": [
    "Mertric Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4c32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _smoothstep(t):\n",
    "    return t * t * (3.0 - 2.0 * t)\n",
    "\n",
    "def _gate(x, left=0.0, right=0.2):\n",
    "    t = (x - left) / (right - left)\n",
    "    t = np.clip(t, 0.0, 1.0)\n",
    "    return _smoothstep(t)\n",
    "\n",
    "def weighted_cosine(pred_flat, true_flat, left=0.0, right=0.2, eps=1e-12):\n",
    "    pred = np.asarray(pred_flat, np.float64).ravel()\n",
    "    true = np.asarray(true_flat, np.float64).ravel()\n",
    "    x = np.maximum(np.abs(pred), np.abs(true))\n",
    "    w = _gate(x, left, right)\n",
    "    w2 = w * w\n",
    "    num = np.sum(w2 * pred * true)\n",
    "    den = np.sqrt(np.sum(w2 * pred * pred)) * np.sqrt(np.sum(w2 * true * true))\n",
    "    return 0.0 if den < eps else float(num / den)\n",
    "\n",
    "def official_score_arrays(y_true, y_pred, w, baseline_wmae, eps=1e-12, max_log2=5.0, cos_left=0.0, cos_right=0.2):\n",
    "    y_true = np.asarray(y_true, np.float64)\n",
    "    y_pred = np.asarray(y_pred, np.float64)\n",
    "    w = np.asarray(w, np.float64)\n",
    "    baseline_wmae = np.asarray(baseline_wmae, np.float64)\n",
    "\n",
    "    pred_wmae = np.mean(np.abs(y_true - y_pred) * w, axis=1)\n",
    "    pred_wmae = np.maximum(pred_wmae, eps)\n",
    "    baseline = np.maximum(baseline_wmae, eps)\n",
    "\n",
    "    terms = np.log2(baseline / pred_wmae)\n",
    "    terms = np.minimum(terms, max_log2)\n",
    "    sum_wmae = float(np.sum(terms))\n",
    "\n",
    "    wcos = weighted_cosine(y_pred.ravel(), y_true.ravel(), left=cos_left, right=cos_right, eps=eps)\n",
    "    return float(sum_wmae * max(0.0, wcos))\n",
    "\n",
    "def proxy_weights_from_deltas(D):\n",
    "    # Approx gene weights from typical movement across perturbations\n",
    "    w = np.mean(np.abs(D), axis=0).astype(np.float64)\n",
    "    w = w / (np.mean(w) + 1e-12)  # mean=1 => sum=n_genes\n",
    "    return w\n",
    "\n",
    "def proxy_score_oof(y_true, y_pred, base_pred_per_row, w_gene):\n",
    "    y_true = np.asarray(y_true, np.float64)\n",
    "    y_pred = np.asarray(y_pred, np.float64)\n",
    "    base_pred_per_row = np.asarray(base_pred_per_row, np.float64)\n",
    "\n",
    "    n, d = y_true.shape\n",
    "    w = np.tile(w_gene[None, :], (n, 1))\n",
    "    baseline_wmae = np.mean(np.abs(y_true - base_pred_per_row) * w, axis=1)\n",
    "    return official_score_arrays(y_true, y_pred, w, baseline_wmae)\n",
    "\n",
    "def soft_threshold(x, t):\n",
    "    if t <= 0:\n",
    "        return x\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - t, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d58dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_genept_for_union(pkl_path, union_genes, pca_dim=128, seed=6):\n",
    "    print(f\"[GenePT] Loading: {pkl_path}\")\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "\n",
    "    dim = None\n",
    "    for g in union_genes:\n",
    "        v = d.get(g.upper(), None)\n",
    "        if v is not None:\n",
    "            vv = np.asarray(v, dtype=np.float32).ravel()\n",
    "            dim = vv.shape[0]\n",
    "            break\n",
    "    if dim is None:\n",
    "        raise ValueError(\"GenePT dict does not contain any of your union genes.\")\n",
    "\n",
    "    E = np.zeros((len(union_genes), dim), dtype=np.float32)\n",
    "    found_vecs = []\n",
    "    missing = 0\n",
    "\n",
    "    for i, g in enumerate(union_genes):\n",
    "        v = d.get(g.upper(), None)\n",
    "        if v is None:\n",
    "            missing += 1\n",
    "            continue\n",
    "        vv = np.asarray(v, dtype=np.float32).ravel()\n",
    "        if vv.shape[0] != dim:\n",
    "            missing += 1\n",
    "            continue\n",
    "        E[i] = vv\n",
    "        found_vecs.append(vv)\n",
    "\n",
    "    fallback = np.mean(np.stack(found_vecs, axis=0), axis=0).astype(np.float32)\n",
    "\n",
    "    mask = np.all(E == 0.0, axis=1)\n",
    "    E[mask] = fallback\n",
    "\n",
    "    print(f\"[GenePT] dim={dim} missing={missing}/{len(union_genes)}\")\n",
    "\n",
    "    pca = PCA(n_components=min(pca_dim, dim), random_state=seed)\n",
    "    E_small = pca.fit_transform(E).astype(np.float32)\n",
    "    gene2 = {g.upper(): E_small[i] for i, g in enumerate(union_genes)}\n",
    "    fb = E_small.mean(axis=0).astype(np.float32)\n",
    "    return E_small, gene2, fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe0112e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_load_tfidf_embeddings(union_genes_upper):\n",
    "    \"\"\"\n",
    "    Builds TF-IDF char ngram embeddings from the provided JSON summaries.\n",
    "    Caches to external_genept/tfidf_combined_svd128.npz\n",
    "    \"\"\"\n",
    "    out_path = os.path.join(GENEPT_DIR, f\"tfidf_combined_svd{TFIDF_SVD_DIM}.npz\")\n",
    "    if os.path.exists(out_path):\n",
    "        z = np.load(out_path, allow_pickle=True)\n",
    "        genes = [str(x).upper() for x in z[\"genes\"]]\n",
    "        emb = z[\"emb\"].astype(np.float32)\n",
    "        mp = {g: emb[i] for i, g in enumerate(genes)}\n",
    "        fb = emb.mean(axis=0).astype(np.float32)\n",
    "        print(\"[TFIDF] Loaded cached:\", out_path, \"shape:\", emb.shape)\n",
    "        return mp, fb, emb.shape[1]\n",
    "\n",
    "    n_path = os.path.join(GENEPT_SUBDIR, \"NCBI_summary_of_genes.json\")\n",
    "    u_path = os.path.join(GENEPT_SUBDIR, \"NCBI_UniProt_summary_of_genes.json\")\n",
    "    if not os.path.exists(n_path) or not os.path.exists(u_path):\n",
    "        raise FileNotFoundError(\"Missing NCBI / UniProt summary JSONs under GenePT_emebdding_v2\")\n",
    "\n",
    "    print(\"[TFIDF] Building embeddings from JSON summaries\")\n",
    "\n",
    "    with open(n_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ncbi = json.load(f)\n",
    "    with open(u_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        unip = json.load(f)\n",
    "\n",
    "    texts = []\n",
    "    miss = 0\n",
    "    for g in union_genes_upper:\n",
    "        tn = ncbi.get(g, \"\") or \"\"\n",
    "        tu = unip.get(g, \"\") or \"\"\n",
    "        t = (str(tn) + \" \" + str(tu)).strip()\n",
    "        if not t:\n",
    "            miss += 1\n",
    "        texts.append(t)\n",
    "\n",
    "    print(\"[TFIDF] empty summaries:\", miss, \"/\", len(union_genes_upper))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer=\"char_wb\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        max_features=200_000,\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    k = min(TFIDF_SVD_DIM, X.shape[0] - 1, X.shape[1] - 1)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=k, random_state=SEED)\n",
    "    E = svd.fit_transform(X).astype(np.float32)\n",
    "\n",
    "    if E.shape[1] < TFIDF_SVD_DIM:\n",
    "        pad = np.zeros((E.shape[0], TFIDF_SVD_DIM - E.shape[1]), dtype=np.float32)\n",
    "        E = np.hstack([E, pad])\n",
    "\n",
    "    np.savez_compressed(out_path, genes=np.array(union_genes_upper, dtype=object), emb=E)\n",
    "    print(\"[TFIDF] wrote:\", out_path, \"shape:\", E.shape, \"explained:\", float(svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "    mp = {g: E[i] for i, g in enumerate(union_genes_upper)}\n",
    "    fb = E.mean(axis=0).astype(np.float32)\n",
    "    return mp, fb, E.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a552395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perts: 80\n",
      "Output genes: 5127\n",
      "Union genes: 5143\n"
     ]
    }
   ],
   "source": [
    "df_means = pd.read_csv(MEANS_PATH)\n",
    "gene_cols = [c for c in df_means.columns if c != \"pert_symbol\"]\n",
    "\n",
    "base_mask = df_means[\"pert_symbol\"].astype(str) == \"non-targeting\"\n",
    "\n",
    "x_base = df_means.loc[base_mask, gene_cols].iloc[0].to_numpy(np.float32)\n",
    "\n",
    "df_train = df_means.loc[~base_mask].reset_index(drop=True)\n",
    "train_genes = df_train[\"pert_symbol\"].astype(str).tolist()\n",
    "\n",
    "D = df_train[gene_cols].to_numpy(np.float32) - x_base[None, :]\n",
    "D_mean = D.mean(axis=0).astype(np.float32)\n",
    "\n",
    "df_valmap = pd.read_csv(VALMAP_PATH)\n",
    "val_map = dict(zip(df_valmap[\"pert_id\"].astype(str), df_valmap[\"pert\"].astype(str)))\n",
    "val_genes = list(val_map.values())\n",
    "\n",
    "union_genes = sorted(set(gene_cols) | set(train_genes) | set(val_genes))\n",
    "union_upper = [g.upper() for g in union_genes]\n",
    "\n",
    "print(\"Train perts:\", len(train_genes))\n",
    "print(\"Output genes:\", len(gene_cols))\n",
    "print(\"Union genes:\", len(union_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f440ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gene = proxy_weights_from_deltas(D)\n",
    "sqrtw = np.sqrt(w_gene).astype(np.float32)\n",
    "sqrtw = sqrtw / (np.mean(sqrtw) + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12905fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad(H5AD_PATH)\n",
    "\n",
    "ctrl = adata[adata.obs[\"sgrna_symbol\"].astype(str) == \"non-targeting\"].copy()\n",
    "q = float(np.quantile(ctrl.obs[\"nCount_RNA\"].values, DEPTH_TRIM_Q))\n",
    "ctrl = ctrl[ctrl.obs[\"nCount_RNA\"].values <= q].copy()\n",
    "\n",
    "ctrl = ctrl[:, union_genes].copy()\n",
    "\n",
    "sc.pp.normalize_total(ctrl, target_sum=1e4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db03ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ctrl.X\n",
    "if not sp.issparse(X):\n",
    "    X = sp.csr_matrix(X)\n",
    "X = X.tocsr(copy=True)\n",
    "X.data = np.log2(X.data + 1.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "580adedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctrl gene emb: (5143, 128)\n"
     ]
    }
   ],
   "source": [
    "X = StandardScaler(with_mean=False).fit_transform(X)\n",
    "X = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "svd_ctrl = TruncatedSVD(n_components=CTRL_SVD_DIM, random_state=SEED)\n",
    "svd_ctrl.fit(X)\n",
    "\n",
    "ctrl_gene_emb = svd_ctrl.components_.T.astype(np.float32)  # (n_union, CTRL_SVD_DIM)\n",
    "gene2ctrl = {g.upper(): ctrl_gene_emb[i] for i, g in enumerate(union_genes)}\n",
    "ctrl_fallback = ctrl_gene_emb.mean(axis=0).astype(np.float32)\n",
    "\n",
    "print(\"ctrl gene emb:\", ctrl_gene_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28c333d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_genept_pickles():\n",
    "    # Prefer the model_3 gene+protein pickle\n",
    "    cand = glob.glob(os.path.join(GENEPT_SUBDIR, \"*.pickle\"))\n",
    "    cand += glob.glob(os.path.join(GENEPT_SUBDIR, \"*.pkl\"))\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"No GenePT pickle found under {GENEPT_SUBDIR}\")\n",
    "\n",
    "    ada = None\n",
    "    m3 = None\n",
    "    for p in cand:\n",
    "        name = os.path.basename(p).lower()\n",
    "        if \"gene_embedding_ada_text\" in name:\n",
    "            ada = p\n",
    "        if \"gene_protein_embedding_model_3\" in name:\n",
    "            m3 = p\n",
    "\n",
    "    # Fallback if names differ\n",
    "    if m3 is None:\n",
    "        m3 = max(cand, key=os.path.getsize)\n",
    "    return os.path.normpath(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d10ca200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'external_genept\\\\GenePT_emebdding_v2\\\\GenePT_gene_protein_embedding_model_3_text.pickle'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_genept_pickles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f19fd8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GenePT] Loading: external_genept\\GenePT_emebdding_v2\\GenePT_gene_protein_embedding_model_3_text.pickle\n",
      "[GenePT] dim=3072 missing=165/5143\n",
      "[GenePT] m3 PCA emb: (5143, 128)\n"
     ]
    }
   ],
   "source": [
    "genept_m3_pkl = 'external_genept\\\\GenePT_emebdding_v2\\\\GenePT_gene_protein_embedding_model_3_text.pickle'\n",
    "E_m3, gene2m3, fb_m3 = load_genept_for_union(genept_m3_pkl, union_genes, pca_dim=GENEPT_PCA_DIM, seed=SEED)\n",
    "\n",
    "print(\"[GenePT] m3 PCA emb:\", E_m3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4049da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIG] emb: (196239, 128) coverage on train: 1 / 80\n"
     ]
    }
   ],
   "source": [
    "sig = np.load(SIG_NPZ, allow_pickle=True)\n",
    "sig_genes = [str(x).upper() for x in sig[\"genes\"]]\n",
    "sig_emb = sig[\"emb\"].astype(np.float32)\n",
    "\n",
    "sig_map = {g: sig_emb[i] for i, g in enumerate(sig_genes)}\n",
    "sig_fb = sig_emb.mean(axis=0).astype(np.float32)\n",
    "\n",
    "print(\"[SIG] emb:\", sig_emb.shape, \"coverage on train:\", sum(g.upper() in sig_map for g in train_genes), \"/\", len(train_genes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab76d6d",
   "metadata": {},
   "source": [
    "TF-IDF text embedding from NCBI + UniProt summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f367c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TFIDF] Loaded cached: external_genept\\tfidf_combined_svd128.npz shape: (5143, 128)\n",
      "[TFIDF] dim: 128\n"
     ]
    }
   ],
   "source": [
    "tf_map, tf_fb, tf_dim = build_or_load_tfidf_embeddings(union_upper)\n",
    "print(\"[TFIDF] dim:\", tf_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38ae6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_for_gene(g):\n",
    "    g = str(g).upper()\n",
    "    a = gene2ctrl.get(g, ctrl_fallback)   # ctrl SVD\n",
    "    b = gene2m3.get(g, fb_m3)             # GenePT m3 PCA\n",
    "    c = sig_map.get(g, sig_fb)            # external perturb signature PCA\n",
    "    d = tf_map.get(g, tf_fb)              # TFIDF-SVD text\n",
    "    return np.concatenate([a, b, c, d], axis=0).astype(np.float32)\n",
    "\n",
    "X_feat = np.vstack([feat_for_gene(g) for g in train_genes]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "593e3c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X] train feature matrix: (80, 512)\n"
     ]
    }
   ],
   "source": [
    "fscaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_feat = fscaler.fit_transform(X_feat).astype(np.float32)\n",
    "\n",
    "print(\"[X] train feature matrix:\", X_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c14f301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dw = (D * sqrtw[None, :]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f41fca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv_raw_preds(K_out, alpha, gamma):\n",
    "    n = Dw.shape[0]\n",
    "    oof_raw_w = np.zeros_like(Dw, dtype=np.float32)\n",
    "    oof_base_w = np.zeros_like(Dw, dtype=np.float32)\n",
    "\n",
    "    for i in range(n):\n",
    "        tr = np.ones(n, dtype=bool)\n",
    "        tr[i] = False\n",
    "\n",
    "        Dw_tr = Dw[tr]\n",
    "        base_tr = Dw_tr.mean(axis=0).astype(np.float32)\n",
    "        oof_base_w[i] = base_tr\n",
    "\n",
    "        pca = PCA(n_components=min(K_out, Dw_tr.shape[0] - 1), random_state=SEED)\n",
    "        C_tr = pca.fit_transform(Dw_tr)\n",
    "\n",
    "        reg = KernelRidge(alpha=alpha, kernel=\"rbf\", gamma=gamma)\n",
    "        reg.fit(X_feat[tr], C_tr)\n",
    "\n",
    "        c_hat = reg.predict(X_feat[i:i+1])\n",
    "        dw_hat = pca.inverse_transform(c_hat)[0].astype(np.float32)\n",
    "        oof_raw_w[i] = dw_hat\n",
    "\n",
    "    # Convert weighted space back to original output space\n",
    "    raw = (oof_raw_w / (sqrtw[None, :] + 1e-12)).astype(np.float32)\n",
    "    base = (oof_base_w / (sqrtw[None, :] + 1e-12)).astype(np.float32)\n",
    "    return raw, base\n",
    "\n",
    "best = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "587524d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'K_out': 32, 'alpha': 0.0001, 'gamma': 0.01, 'scale': 0.75, 'thr': 0.0, 'blend': 1.0, 'proxy': 0.5981081592545646}\n",
      "Best {'K_out': 48, 'alpha': 0.0001, 'gamma': 0.01, 'scale': 0.75, 'thr': 0.0, 'blend': 1.0, 'proxy': 0.6004260299347904}\n",
      "Best {'K_out': 64, 'alpha': 0.0001, 'gamma': 0.01, 'scale': 0.75, 'thr': 0.0, 'blend': 1.0, 'proxy': 0.6014028329746118}\n",
      "\n",
      "Final best: {'K_out': 64, 'alpha': 0.0001, 'gamma': 0.01, 'scale': 0.75, 'thr': 0.0, 'blend': 1.0, 'proxy': 0.6014028329746118}\n"
     ]
    }
   ],
   "source": [
    "for K_out in OUT_K_GRID:\n",
    "    for alpha in ALPHA_GRID:\n",
    "        for gamma in GAMMA_GRID:\n",
    "            raw, base = loocv_raw_preds(K_out, alpha, gamma)\n",
    "\n",
    "            for scale in SCALE_GRID:\n",
    "                for thr in THR_GRID:\n",
    "                    for blend in BLEND_GRID:\n",
    "                        pred = (scale * raw).astype(np.float32)\n",
    "                        pred = soft_threshold(pred, thr).astype(np.float32)\n",
    "                        pred = ((1.0 - blend) * base + blend * pred).astype(np.float32)\n",
    "\n",
    "                        s = proxy_score_oof(D, pred, base, w_gene)\n",
    "                        item = {\n",
    "                            \"K_out\": K_out, \"alpha\": float(alpha), \"gamma\": float(gamma),\n",
    "                            \"scale\": float(scale), \"thr\": float(thr), \"blend\": float(blend),\n",
    "                            \"proxy\": float(s)\n",
    "                        }\n",
    "                        if best is None or item[\"proxy\"] > best[\"proxy\"]:\n",
    "                            best = item\n",
    "                            print(\"Best\", best)\n",
    "\n",
    "print(\"\\nFinal best:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01418740",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_out = int(best[\"K_out\"])\n",
    "alpha = float(best[\"alpha\"])\n",
    "gamma = float(best[\"gamma\"])\n",
    "scale = float(best[\"scale\"])\n",
    "thr   = float(best[\"thr\"])\n",
    "blend = float(best[\"blend\"])\n",
    "\n",
    "pca = PCA(n_components=min(K_out, Dw.shape[0] - 1), random_state=SEED)\n",
    "C = pca.fit_transform(Dw).astype(np.float32)\n",
    "\n",
    "reg = KernelRidge(alpha=alpha, kernel=\"rbf\", gamma=gamma)\n",
    "reg.fit(X_feat, C)\n",
    "\n",
    "def predict_delta(gene_symbol):\n",
    "    x = feat_for_gene(gene_symbol)[None, :].astype(np.float32)\n",
    "    x = fscaler.transform(x).astype(np.float32)\n",
    "\n",
    "    c_hat = reg.predict(x)\n",
    "    dw_hat = pca.inverse_transform(c_hat)[0].astype(np.float32)\n",
    "    d_hat = (dw_hat / (sqrtw + 1e-12)).astype(np.float32)\n",
    "\n",
    "    d_hat = (scale * d_hat).astype(np.float32)\n",
    "    d_hat = soft_threshold(d_hat, thr).astype(np.float32)\n",
    "    d_hat = ((1.0 - blend) * D_mean + blend * d_hat).astype(np.float32)\n",
    "    return d_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72b74c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: model_full_external_ctrl_genept_sig_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for k in range(1, 121):\n",
    "    pert_id = f\"pert_{k}\"\n",
    "    g = val_map.get(pert_id, None)\n",
    "\n",
    "    if g is None:\n",
    "        delta = D_mean\n",
    "    else:\n",
    "        delta = predict_delta(g)\n",
    "\n",
    "    row = {\"pert_id\": pert_id}\n",
    "    row.update({gene_cols[j]: float(delta[j]) for j in range(len(gene_cols))})\n",
    "    rows.append(row)\n",
    "\n",
    "sub = pd.DataFrame(rows, columns=[\"pert_id\"] + gene_cols)\n",
    "\n",
    "out_path = \"model_full_external_ctrl_genept_sig_tfidf.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "print(\"Created:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fad969",
   "metadata": {},
   "source": [
    "Absolute dogshit results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
