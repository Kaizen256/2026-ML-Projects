# ML-Models-on-Datasets
Collection of machine learning projects created using scikit-learn, PyTorch, or implemented from scratch in NumPy. All projects are original, built from the ground up based on my own understanding. No tutorials were used, these are genuine implementations to deepen my grasp of ML concepts. All projects were created as I learned, so a few include less practical decisions, like evaluating an imbalanced classification problem using only accuracy.

# Top Projects
These are projects I am especially proud of. Each projects ipynb file contains fleshed out explanations of my thought process, and the code. This list will evolve as I continue to learn and build more.

| Project                                  | Description                                                                                                                                                                                  | Location                                      |
|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|
| **LeNet-5 on MNIST (NumPy & PyTorch)**   | Implemented LeNet-5 entirely from scratch in NumPy, with manual forward & backward for convolutions, pooling, and dense layers. Later replicated in PyTorch to achieve ~99% on MNIST.        | [`CNN from Scratch with NumPy/`](./CNN_from_Scratch_with_NumPy)           |
| **Swin Transformer from Scratch (PyTorch)**   | Implemented the Shifted-window Transformer architecture, heavily modified for 64×64 Tiny ImageNet, with Relative Position Bias, Windowed Attention, stochastic depth, and CutMix / Mixup.        | [`Swin Transformer/`](./Swin_Transformer)           |
| **Single Shot Multibox Detection (SSD)**     | Built SSD end-to-end in PyTorch on Penn-Fudan: custom GT-centered cropping to replace RandomIoUCrop, ResNet-34 multi-scale backbone, anchor generation (7760/image), IoU matching, hard-negative mining, and NMS inference. | [`Single Shot Multibox Detection/`](./Single_Shot_Multibox_Detection) |
| **English–Japanese GRU Translator (NumPy)** | Encoder–decoder model built entirely in NumPy to translate short English sentences into Japanese. Manual GRU forward/backward, teacher forcing, gradient clipping, and backpropagation calculations written on paper.  | [`GRU Translator/`](./GRU_Encoder_Decoder_Numpy) |
| **GoogLeNet from Scratch (Tiny ImageNet)** | Built Inception v1 from scratch in PyTorch, adapted for 64×64 Tiny ImageNet by modifying pooling & convolutions. Included hand-drawn architecture diagrams and tensor checks.    | [`GoogLeNet from Scratch/`](./GoogLeNet_from_Scratch_with_Pytorch) |
| **Transformer English–Japanese Translator (PyTorch)** | Implemented full Transformer encoder–decoder model from scratch in PyTorch, including attention, positional encodings, masking, and a custom LR scheduler. Outputs generated via beam search. | [`Transformer Translator/`](./Transformer_Encoder-Decoder) |
| **Stellar Classification (SDSS)**        | Classified stars, galaxies, and quasars using Sloan Digital Sky Survey data. Engineered rich photometric features, handled imbalance with SMOTE, tuned CatBoost/LightGBM/XGBoost to ~98.5%.  | [`Stellar Classification/`](./Stellar_Classification) |
| **Tiny ImageNet ResNet-34 (PyTorch)**    | Modified ResNet-34, and built from scratch in Pytorch for small 64×64 Tiny ImageNet images by removing early pooling and adjusting initial convolutions. Trained on Kaggle GPUs to ~61% top-1 accuracy.                         | [`ResNet-34 from Scratch/`](./ResNet-34_from_Scratch_with_Pytorch)   |
| **Invariant Mass from Dielectron Events**| Predicted invariant mass of electron pairs from CMS proton-proton collisions (CERN). Used physics-driven features & XGBoost tuned to ~0.997 R² on test data.                                 | [`Invariant Mass CERN/`](./Invariant_Mass_CERN) |